#!/usr/bin/env python3

import os
import sys
import hashlib
import csv
import time
import yara
from verification_data import verification_data  # generated by rule_generator.py

# ----------------------------------------------------------------------
# Progress display
# ----------------------------------------------------------------------
class Progress:
    def __init__(self, total_files=None):
        self.total_files = total_files
        self.scanned = 0
        self.matches = 0
        self.start_time = time.time()
        self.last_update = 0
    
    def update(self, scanned=None, matches=None, current_dir=None):
        if scanned is not None:
            self.scanned = scanned
        if matches is not None:
            self.matches = matches
        now = time.time()
        if now - self.last_update > 2 or (self.total_files and self.scanned % 500 == 0):
            self.last_update = now
            elapsed = now - self.start_time
            rate = self.scanned / elapsed if elapsed > 0 else 0
            print(f"\r[INFO] Scanned: {self.scanned:,} files | Matches: {self.matches} | "
                  f"Rate: {rate:.1f} files/s", end='')
            if current_dir:
                # Truncate long paths for display
                if len(current_dir) > 50:
                    current_dir = "..." + current_dir[-47:]
                print(f" | Scanning: {current_dir}", end='')
            if self.total_files:
                pct = (self.scanned / self.total_files) * 100
                print(f" | [PROGRESS] {pct:.1f}%", end='')
            print(" " * 20, end='')  # clear leftover
            sys.stdout.flush()

# ----------------------------------------------------------------------
# Hash verification
# ----------------------------------------------------------------------
def verify_file(filepath, expected_md5, expected_sha1, expected_size):
    """Streaming hash verification."""
    try:
        size = os.path.getsize(filepath)
        if size != expected_size:
            return False
        md5 = hashlib.md5()
        sha1 = hashlib.sha1()
        with open(filepath, 'rb') as f:
            while chunk := f.read(64*1024):
                md5.update(chunk)
                sha1.update(chunk)
        return md5.hexdigest().upper() == expected_md5 and sha1.hexdigest().upper() == expected_sha1
    except:
        return False

# ----------------------------------------------------------------------
# Main scanner
# ----------------------------------------------------------------------
def scan_path(start_path, rules, progress):
    """
    Recursively scan from start_path (no directory filtering).
    Returns list of matches.
    """
    results = []
    
    # Count total files for better progress (optional)
    print(f"[INFO] Counting files in {start_path}...")
    total_files = 0
    try:
        for root, dirs, files in os.walk(start_path):
            total_files += len(files)
            if total_files % 1000 == 0:
                print(f"[INFO] Found {total_files:,} files so far...", end='\r')
    except Exception as e:
        print(f"\n[WARNING] Could not count all files: {e}")
        total_files = None
    
    if total_files:
        print(f"\n[INFO] Total files to scan: {total_files:,}")
        progress.total_files = total_files
    
    # Actual scanning â€“ no directory filtering
    scanned = 0
    for root, dirs, files in os.walk(start_path):
        for file in files:
            filepath = os.path.join(root, file)
            scanned += 1
            
            try:
                progress.update(scanned=scanned, current_dir=root)
                
                # Read first up to 50 bytes (may be less for small files)
                with open(filepath, 'rb') as f:
                    first_bytes = f.read(50)

                # YARA match works on whatever we have
                matches = rules.match(data=first_bytes)
                if not matches:
                    continue
                
                # Candidate found, verify with hash
                with open(filepath, 'rb') as f:
                    md5 = hashlib.md5()
                    sha1 = hashlib.sha1()
                    size = 0
                    while chunk := f.read(64*1024):
                        md5.update(chunk)
                        sha1.update(chunk)
                        size += len(chunk)
                    md5_digest = md5.hexdigest().upper()
                
                if md5_digest in verification_data:
                    info = verification_data[md5_digest]
                    if sha1.hexdigest().upper() == info['sha1'] and size == info['file_size']:
                        progress.update(matches=progress.matches+1)
                        file_type = info['file_type']
                        magic = ' '.join(f'{b:02X}' for b in first_bytes[:16]) + '...'
                        results.append({
                            'File Name': file,
                            'Hash MD5': md5_digest,
                            'Hash SHA1': info['sha1'],
                            'Directory': root,
                            'File Type': file_type,
                            'Magic Bytes': magic
                        })
                        print(f"\n[FOUND] {file} ({file_type}) in {root}")
            except PermissionError:
                continue
            except Exception as e:
                # Optionally log other errors
                # print(f"\n[ERROR] {filepath}: {e}")
                continue
    
    return results

def get_all_drives():
    """Return list of drive roots on Windows."""
    drives = []
    if sys.platform == 'win32':
        import string
        from ctypes import windll
        drive_mask = windll.kernel32.GetLogicalDrives()
        for letter in string.ascii_uppercase:
            if drive_mask & 1:
                drives.append(f"{letter}:\\")
            drive_mask >>= 1
    else:
        # Unix-like: start from /
        drives = ['/']
    return drives

def main():
    # Parse command line arguments
    if len(sys.argv) > 1:
        scan_targets = [sys.argv[1]]
        print(f"[INFO] Scanning specified path: {scan_targets[0]}")
    else:
        scan_targets = get_all_drives()
        print(f"[INFO] No path specified. Scanning all drives: {scan_targets}")
    
    # Compile YARA rules
    try:
        rules = yara.compile(filepath='signatures.yar')
        print("[INFO] YARA rules compiled successfully")
    except Exception as e:
        print(f"[ERROR] Error compiling YARA rules: {e}")
        print("[INFO] Make sure signatures.yar exists in the current directory")
        sys.exit(1)
    
    # Check if verification data exists
    if not verification_data:
        print("[ERROR] verification_data is empty. Run rule_generator.py first.")
        sys.exit(1)
    
    print(f"[INFO] Loaded {len(verification_data)} hash entries for verification")
    
    progress = Progress()
    all_results = []
    
    for target in scan_targets:
        if not os.path.exists(target):
            print(f"[WARNING] Path does not exist: {target}")
            continue
            
        print(f"\n{'='*60}")
        print(f"[INFO] Starting scan of {target}")
        print(f"{'='*60}")
        
        results = scan_path(target, rules, progress)
        all_results.extend(results)
    
    print(f"\n{'='*60}")
    print("[INFO] Scan completed.")
    print(f"{'='*60}")
    
    # Sort results according to specified order
    type_order = {
        'JPEG-JFIF': 1, 'JPEG-EXIF': 2, 'JPEG-APP0': 3, 'JPEG-APP1': 4, 'JPEG': 5,
        'PNG': 6, 'MODIFIED-PNG': 7,
        'PDF-1.3': 8, 'PDF-1.4': 9, 'PDF-1.5': 10, 'PDF-1.6': 11, 'PDF-1.7': 12, 'PDF': 13,
        'EXE': 14,
        'ZIP': 15,
        'UTF8-TEXT': 16, 'BATCH': 17, 'ID3': 18, 'RIFF': 19,
        'CUSTOM-NULLPAD': 20, 'CUSTOM-HIGHENT': 21, 'CUSTOM-ASCIIDOM': 22, 'CUSTOM-UNKNOWN-BIN': 23
    }
    def sort_key(entry):
        return (type_order.get(entry['File Type'], 99), entry['File Name'].lower())
    
    all_results.sort(key=sort_key)
    
    # Write CSV
    csv_filename = 'MP1_Scan_Results.csv'
    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['File Name', 'Hash MD5', 'Hash SHA1', 'Directory', 'File Type', 'Magic Bytes']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for res in all_results:
            writer.writerow(res)
    
    print(f"\n[INFO] Results written to {csv_filename}")
    print(f"[INFO] Total matches found: {len(all_results)}")
    
    if all_results:
        print(f"\n{'='*60}")
        print("SUMMARY BY FILE TYPE:")
        print(f"{'='*60}")
        type_counts = {}
        for res in all_results:
            type_counts[res['File Type']] = type_counts.get(res['File Type'], 0) + 1
        for ft, count in sorted(type_counts.items(), key=lambda x: type_order.get(x[0], 99)):
            print(f"  {ft}: {count} files")
        print(f"{'='*60}")

if __name__ == '__main__':
    main()